{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9151732,"sourceType":"datasetVersion","datasetId":5528254},{"sourceId":9155098,"sourceType":"datasetVersion","datasetId":5530511},{"sourceId":9218438,"sourceType":"datasetVersion","datasetId":5574658},{"sourceId":95484,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":80079,"modelId":104549}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Path to the outputs directory\noutputs_dir = \"outputs\"\n\n# Remove the outputs directory if it exists\nif os.path.exists(outputs_dir):\n    shutil.rmtree(outputs_dir)\n    print(f\"Deleted directory: {outputs_dir}\")\nelse:\n    print(f\"Directory {outputs_dir} does not exist.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-deps packaging ninja einops trl peft accelerate bitsandbytes\n!pip install transformers datasets\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install xformers==0.0.23\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets transformers \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install triton ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"HF_TOKEN\"] = \"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\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Organized version ","metadata":{}},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers<0.0.24\" \"trl<0.9.0\" peft accelerate bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nmajor_version, minor_version = torch.cuda.get_device_capability()\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\nif major_version >= 8:\n    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\nelse:\n    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n    !pip install --no-deps xformers trl peft accelerate bitsandbytes\npass\n!pip install triton transformers\n!pip install -U datasets\n!pip install xformers==0.0.23\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n\n# Configuration\nmax_seq_length = 1024\ndtype = None\nload_in_4bit = True\n\n# Load the model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0.1,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\nEOS_TOKEN = tokenizer.eos_token \n# Load and prepare the dataset\ndataset_path = '/kaggle/input/cleanedqaesg/cleaned_esg_dataset.jsonl'\nraw_dataset = load_dataset('json', data_files=dataset_path)\n\ndef format_dataset(example):\n    instruction = \"Answer the following question related to ESG:\"\n    input_text = example['input']\n    output_text = example['output']\n    formatted_input = f\"{instruction}\\n\\nQuestion: {input_text}\\n\\nAnswer: {output_text}\"\n    return {'text': formatted_input}\n\n\ndataset = raw_dataset['train'].map(format_dataset, batched=False)\ndataset = dataset.train_test_split(test_size=0.1)\n\n# Training Arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=60,\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"./outputs_esg_final\",\n)\n\n# Trainer initialization\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['test'],\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,\n    args=training_args,\n)\n\n# GPU memory usage before training\nstart_memory = torch.cuda.memory_reserved(0)\nstart_memory_gb = start_memory / (1024 ** 3)\ngpu_properties = torch.cuda.get_device_properties(0)\ntotal_memory_gb = gpu_properties.total_memory / (1024 ** 3)\nprint(f\"GPU: {gpu_properties.name}, Total Memory: {total_memory_gb:.2f} GB\")\nprint(f\"Initial Memory Reserved: {start_memory_gb:.2f} GB\")\n\n# Train the model\ntrainer_stats = trainer.train()\n\n# GPU memory usage after training\nend_memory = torch.cuda.memory_reserved(0)\nend_memory_gb = end_memory / (1024 ** 3)\nmemory_used_gb = end_memory_gb - start_memory_gb\nmemory_used_percentage = (end_memory / gpu_properties.total_memory) * 100\n\n# Training stats and memory usage\ntrain_time_seconds = trainer_stats.metrics['train_runtime']\ntrain_time_minutes = train_time_seconds / 60\nprint(f\"Training Time: {train_time_seconds:.2f} seconds ({train_time_minutes:.2f} minutes)\")\nprint(f\"Peak Memory Reserved: {end_memory_gb:.2f} GB\")\nprint(f\"Memory Used for Training: {memory_used_gb:.2f} GB\")\nprint(f\"Memory Used Percentage: {memory_used_percentage:.2f}%\")\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Prepare the model for inference\nFastLanguageModel.for_inference(model)\n\n# Define the sample input text for testing\ninput_text = \"What are the key ESG reporting standards companies should follow?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n\n# Generate the output from the model\noutputs = model.generate(**inputs, max_length=1024, early_stopping=True)\n\n# Decode the generated output\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Print the generated response\nprint(\"Generated Response:\")\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Set your Hugging Face token in the environment variables\nos.environ[\"HF_TOKEN\"] = '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'  # Replace with your actual Hugging Face token\n\n# Import necessary components\nfrom unsloth import FastLanguageModel\n# Save only the LoRA adapters locally\nadapter_save_path = \"./lora_adapters\"\nmodel.save_pretrained(adapter_save_path)\nprint(f\"Adapters saved locally at {adapter_save_path}.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Push the LoRA adapters to Hugging Face Hub\nadapter_repo_name = \"llama3-esg-8b-lora-adapters_finalchat\"\ntry:\n    model.push_to_hub(\n        repo_id=f\"AchrafGhribi31/{adapter_repo_name}\",\n        tokenizer=tokenizer, \n        save_method=\"lora\", \n        token=os.environ.get(\"HF_TOKEN\")\n    )\n    print(f\"LoRA adapters successfully pushed to Hugging Face Hub: {adapter_repo_name}\")\nexcept Exception as e:\n    print(f\"Failed to push the LoRA adapters to Hugging Face: {e}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the LoRA adapters with the base model using 4-bit precision and push directly to the Hugging Face Hub\nmerged_repo_name = \"llama3-esg-8b-merged-4bit_finalchat\"\ntry:\n    model.push_to_hub_merged(\n        repo_id=f\"AchrafGhribi31/{merged_repo_name}\",\n        tokenizer=tokenizer, \n        save_method=\"merged_4bit_forced\", \n        token=os.environ.get(\"HF_TOKEN\")\n    )\n    print(f\"Merged model (4-bit) successfully pushed to Hugging Face Hub: {merged_repo_name}\")\nexcept Exception as e:\n    print(f\"Failed to push the merged model (4-bit) to Hugging Face: {e}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparison Fine_tuned model VS Non Fine_tuned model  ","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\n# Load the non-fine-tuned model\nbase_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\nbase_model, base_tokenizer = FastLanguageModel.from_pretrained(\n    model_name=base_model_name,\n    max_seq_length=1024,\n    load_in_4bit=True,  # Load the model in 4-bit precision\n)\n\n# Prepare the base model for inference\nbase_model = FastLanguageModel.for_inference(base_model)\n\n# Load the fine-tuned model\nfine_tuned_model_name = \"AchrafGhribi31/llama3-esg-8b-merged-4bit_finalchat\"\nfine_tuned_model, fine_tuned_tokenizer = FastLanguageModel.from_pretrained(\n    model_name=fine_tuned_model_name,\n    max_seq_length=1024,\n    load_in_4bit=True,  # Load the model in 4-bit precision\n)\n\n# Prepare the fine-tuned model for inference\nfine_tuned_model = FastLanguageModel.for_inference(fine_tuned_model)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-22T14:15:09.836994Z","iopub.execute_input":"2024-08-22T14:15:09.837697Z","iopub.status.idle":"2024-08-22T14:16:29.424528Z","shell.execute_reply.started":"2024-08-22T14:15:09.837663Z","shell.execute_reply":"2024-08-22T14:16:29.423383Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/unsloth/__init__.py:110: UserWarning: Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2024-08-22 14:15:18.198471: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-22 14:15:18.198583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-22 14:15:18.302481: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.1.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.23. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b615ece50ed491e8446496634eecd3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"699b44dc55b3411b963dc7717b22ebef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56181d133583465891e2643b504bd9b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e18edea3dcae4e468ea068fc759cb485"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b8817f8cfc45fc8bda5dba206747f1"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.1.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.23. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/132k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f83d1a98c44d658eddbd82525b82fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d8d323b3a24845bfbf13805773825e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b947bb2ad2ea44b9983849645effda50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84b597064b344d83accbc7141c00a58d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7459809666a9421e95173a574acb1697"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6267875fd41f4191afe708ff0e5e2bf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdbcc0cb85004980a34a09e1f5d9cae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb4a82545ffb4a0491cc3b32450e6976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf5d51a8a0a4be2a87f5248860f976a"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom difflib import SequenceMatcher\nimport numpy as np\nimport pandas as pd\n\n# Load the original data\ndataset_path = '/kaggle/input/cleanedqaesg/cleaned_esg_dataset.jsonl'\nraw_dataset = load_dataset('json', data_files=dataset_path)\n\ndef format_dataset(example):\n    instruction = \"Answer the following question related to ESG:\"\n    input_text = example['input']\n    output_text = example['output']\n    formatted_input = f\"{instruction}\\n\\nQuestion: {input_text}\\n\\nAnswer: {output_text}\"\n    return {'input': input_text, 'output': output_text, 'text': formatted_input}\n\ndataset = raw_dataset['train'].map(format_dataset, batched=False)\ndataset = dataset.train_test_split(test_size=0.1)\ntest_dataset = dataset['test']\n\n# Ensure your model and tokenizer are on the GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Function to generate predictions from the model\ndef generate_prediction(model, tokenizer, question):\n    inputs = tokenizer(question, return_tensors='pt').to(device)\n    with torch.autocast(device_type='cuda', dtype=torch.float16):\n        outputs = model.generate(**inputs, max_new_tokens=50)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n\n# Function to calculate perplexity\ndef calculate_perplexity(model, tokenizer, text):\n    inputs = tokenizer(text, return_tensors='pt').to(device)\n    input_ids = inputs['input_ids']\n    labels = input_ids.clone()\n    with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16):\n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n    perplexity = torch.exp(loss).item()\n    return perplexity\n\n# Function to calculate token-level precision, recall, and F1-score\ndef token_level_f1(prediction, reference):\n    pred_tokens = prediction.split()\n    ref_tokens = reference.split()\n    if not pred_tokens or not ref_tokens:\n        return 0, 0, 0  # Avoid NaN by returning zero when there are no tokens\n    correct = sum((1 for token in pred_tokens if token in ref_tokens))\n    precision = correct / len(pred_tokens) if pred_tokens else 0\n    recall = correct / len(ref_tokens) if ref_tokens else 0\n    if precision + recall > 0:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1 = 0\n    return precision, recall, f1\n\n# Lists to store evaluation results\nbase_model_results = {\"f1_score\": [], \"bleu\": [], \"perplexity\": []}\nfine_tuned_model_results = {\"f1_score\": [], \"bleu\": [], \"perplexity\": []}\n\n# Loop through the test dataset and evaluate both models\nfor example in test_dataset:\n    question = example['input']\n    reference_answer = example['output']\n\n    # Base model predictions\n    base_pred = generate_prediction(base_model, base_tokenizer, question)\n    # Fine-tuned model predictions\n    fine_tuned_pred = generate_prediction(fine_tuned_model, fine_tuned_tokenizer, question)\n\n    # Debugging output for predictions\n    print(f\"Reference: {reference_answer}\")\n    print(f\"Base Model Prediction: {base_pred}\")\n    print(f\"Fine-Tuned Model Prediction: {fine_tuned_pred}\")\n\n    # Token-level F1-score (including precision and recall)\n    _, _, base_f1 = token_level_f1(base_pred, reference_answer)\n    _, _, fine_tuned_f1 = token_level_f1(fine_tuned_pred, reference_answer)\n\n    base_model_results[\"f1_score\"].append(base_f1)\n    fine_tuned_model_results[\"f1_score\"].append(fine_tuned_f1)\n\n    # BLEU Score, checking for empty predictions or references\n    if base_pred.strip() and reference_answer.strip():\n        base_bleu = sentence_bleu([reference_answer.split()], base_pred.split())\n    else:\n        base_bleu = 0\n\n    if fine_tuned_pred.strip() and reference_answer.strip():\n        fine_tuned_bleu = sentence_bleu([reference_answer.split()], fine_tuned_pred.split())\n    else:\n        fine_tuned_bleu = 0\n\n    base_model_results[\"bleu\"].append(base_bleu)\n    fine_tuned_model_results[\"bleu\"].append(fine_tuned_bleu)\n\n    # Perplexity\n    base_model_results[\"perplexity\"].append(calculate_perplexity(base_model, base_tokenizer, reference_answer))\n    fine_tuned_model_results[\"perplexity\"].append(calculate_perplexity(fine_tuned_model, fine_tuned_tokenizer, reference_answer))\n\n# Average the results across all examples in the dataset\nbase_model_metrics = {metric: np.mean(values) for metric, values in base_model_results.items()}\nfine_tuned_model_metrics = {metric: np.mean(values) for metric, values in fine_tuned_model_results.items()}\n\n# Print the results in a table for better visualization\nmetrics = [\"F1-Score\", \"BLEU Score\", \"Perplexity\"]\n\nmetrics_comparison = pd.DataFrame({\n    \"Metric\": metrics,\n    \"Base Model\": [base_model_metrics.get(m.lower(), None) for m in metrics],\n    \"Fine-Tuned Model\": [fine_tuned_model_metrics.get(m.lower(), None) for m in metrics]\n})\n\n# Display the table\nprint(metrics_comparison)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef generate_predictions_batch(model, tokenizer, dataset, batch_size=8, max_len=512):\n    \"\"\"\n    Generates predictions in batches from the given dataset using the specified model and tokenizer.\n\n    Parameters:\n    - model: The language model for prediction.\n    - tokenizer: The tokenizer associated with the model.\n    - dataset: The dataset to generate predictions from.\n    - batch_size: The number of examples to process in each batch.\n    - max_len: The maximum length of the generated sequence.\n\n    Returns:\n    - predictions: A list of generated predictions.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    predictions = []\n\n    with torch.no_grad():  # Disable gradient tracking\n        for i in range(0, len(dataset), batch_size):\n            batch_inputs = dataset[i:i + batch_size]\n            input_texts = [example['input'] for example in batch_inputs]\n\n            # Tokenize the batch of input texts\n            inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n            # Generate predictions\n            outputs = model.generate(**inputs, max_length=max_len, early_stopping=True)\n\n            # Decode the generated outputs\n            batch_predictions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n            predictions.extend(batch_predictions)\n\n    return predictions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming `formatted_validation_dataset` is already prepared\nbatch_size = 8\nmax_len = 512\n\n# Generate predictions using the base model\nbase_model_predictions = generate_predictions_batch(base_model, base_tokenizer, formatted_validation_dataset, batch_size, max_len)\n\n# Generate predictions using the fine-tuned model\nfine_tuned_model_predictions = generate_predictions_batch(fine_tuned_model, fine_tuned_tokenizer, formatted_validation_dataset, batch_size, max_len)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\n\nrouge_metric = load_metric(\"rouge\")\n\n# Evaluate base model\nbase_model_scores = rouge_metric.compute(\n    predictions=base_model_predictions,\n    references=[example[\"output\"] for example in formatted_validation_dataset]\n)\n\n# Evaluate fine-tuned model\nfine_tuned_model_scores = rouge_metric.compute(\n    predictions=fine_tuned_model_predictions,\n    references=[example[\"output\"] for example in formatted_validation_dataset]\n)\n\nprint(\"Base Model ROUGE Scores:\", base_model_scores)\nprint(\"Fine-Tuned Model ROUGE Scores:\", fine_tuned_model_scores)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Convert ROUGE scores to a more readable table format\ndef rouge_scores_to_table(base_scores, fine_tuned_scores):\n    rows = []\n    for metric in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']:\n        row = {\n            'Metric': metric.upper(),\n            'Base Precision': base_scores[metric].mid.precision,\n            'Base Recall': base_scores[metric].mid.recall,\n            'Base F1': base_scores[metric].mid.fmeasure,\n            'Fine-Tuned Precision': fine_tuned_scores[metric].mid.precision,\n            'Fine-Tuned Recall': fine_tuned_scores[metric].mid.recall,\n            'Fine-Tuned F1': fine_tuned_scores[metric].mid.fmeasure\n        }\n        rows.append(row)\n\n    df = pd.DataFrame(rows)\n    return df\n\n# Create the table\nrouge_table = rouge_scores_to_table(base_model_scores, fine_tuned_model_scores)\n\n# Display the table\nprint(rouge_table)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation of our fined tuned 4bit Llama3 :","metadata":{}},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset, load_metric\nfrom tqdm import tqdm\nimport os\n\n# Load the merged 4-bit fine-tuned model from the Hugging Face Hub\nmodel_name = \"AchrafGhribi31/llama3-esg-8b-merged-4bit_V0\"\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=1024,\n    load_in_4bit=True,  # Load the model in 4-bit precision\n    token=os.environ.get(\"HF_TOKEN\")  # Add your HF token if the model is private\n)\n\n# Set the model to evaluation mode\nmodel.eval()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the entire dataset\ndataset_path = '/kaggle/input/chatllama/combined_esg_dataset.jsonl'\nraw_dataset = load_dataset('json', data_files=dataset_path)\n\n# Split the dataset into train and test sets (e.g., 90% train, 10% test)\ndataset = raw_dataset['train'].train_test_split(test_size=0.1, seed=42)\n\n# Use the 'test' split for evaluation\neval_dataset = dataset['test']\n\n# Format the dataset if necessary\ndef format_dataset(example):\n    input_text = example['input']\n    expected_output = example['output']\n    return {'input_text': input_text, 'expected_output': expected_output}\n\neval_dataset = eval_dataset.map(format_dataset, batched=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge-score evaluate\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nimport evaluate\nfrom tqdm import tqdm\nimport re\nimport os \n# Load the merged 4-bit fine-tuned model from the Hugging Face Hub\nmodel_name = \"AchrafGhribi31/llama3-esg-8b-merged-4bit_V0\"\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=1024,\n    load_in_4bit=True,  # Load the model in 4-bit precision\n    token=os.environ.get(\"HF_TOKEN\")  # Add your HF token if the model is private\n)\n\n# Prepare the model for inference\nFastLanguageModel.for_inference(model)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the evaluation metrics\nrouge_metric = evaluate.load(\"rouge\")\nbleu_metric = evaluate.load(\"bleu\")\n\n# Clean the text function\ndef clean_text(text):\n    # Remove instruction tokens, system messages, and extra tokens\n    text = re.sub(r'\\[INST\\]', '', text)\n    text = re.sub(r'\\[\\/INST\\]', '', text)\n    text = re.sub(r'<<SYS>>.*?<</SYS>>', '', text, flags=re.DOTALL)\n    text = re.sub(r'<.*?>', '', text)  # Remove other potential special tokens\n    text = text.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").strip()\n    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with spaces\n    return text\n\n# Evaluation function\ndef evaluate_example(input_text, expected_output):\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=1024)\n    generated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Clean up the generated and expected outputs\n    generated_output_cleaned = clean_text(generated_output)\n    expected_output_cleaned = clean_text(expected_output)\n\n    # Debugging: Print cleaned outputs to check for issues\n    print(f\"Generated: {generated_output_cleaned}\")\n    print(f\"Expected: {expected_output_cleaned}\")\n    print(f\"Length of generated output: {len(generated_output_cleaned)}\")\n    print(f\"Length of expected output: {len(expected_output_cleaned)}\")\n\n    # Check types and contents of predictions and references\n    print(f\"Generated output type: {type(generated_output_cleaned)}\")\n    print(f\"Expected output type: {type(expected_output_cleaned)}\")\n    print(f\"Generated output as list: {list(generated_output_cleaned)}\")\n    print(f\"Expected output as list: {list(expected_output_cleaned)}\")\n\n    # Compute ROUGE and BLEU scores\n    try:\n        rouge_metric.add(predictions=[generated_output_cleaned], references=[expected_output_cleaned])\n        bleu_metric.add(predictions=[generated_output_cleaned.split()], references=[[expected_output_cleaned.split()]])\n    except Exception as e:\n        print(f\"Error in adding metrics: {e}\")\n        print(f\"Generated Output: {generated_output_cleaned}\")\n        print(f\"Expected Output: {expected_output_cleaned}\")\n        raise e\n\n    return generated_output_cleaned\n\n# Load and evaluate on the dataset\ndataset_path = '/kaggle/input/chatllama/combined_esg_dataset.jsonl'\neval_dataset = load_dataset('json', data_files=dataset_path, split='train')\n\npredictions, references = [], []\nfor example in tqdm(eval_dataset):\n    input_text = example['input']\n    expected_output = example['output']\n    generated_output = evaluate_example(input_text, expected_output)\n    \n    predictions.append(generated_output)\n    references.append(expected_output)\n\n# Compute metrics\nrouge_scores = rouge_metric.compute()\nbleu_scores = bleu_metric.compute()\n\n# Display results\nprint(\"ROUGE Scores:\")\nfor key, value in rouge_scores.items():\n    print(f\"  {key}: {value['f']:.4f}\")\n\nprint(f\"BLEU Score: {bleu_scores['bleu']:.4f}\")\n\n# Save results\nresults = {\n    \"rouge_scores\": rouge_scores,\n    \"bleu_scores\": bleu_scores,\n    \"predictions\": predictions,\n    \"references\": references,\n}\n\nimport json\nwith open(f\"./{model_name}-evaluation_results.json\", \"w\") as f:\n    json.dump(results, f, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the model for inference\nFastLanguageModel.for_inference(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing the fine-tuned model with a sample question\ninput_text = \"How does a company's ESG performance impact its financial performance?\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=1024)\n# Adjusting inference parameters\nresponse = model.generate(\n    **inputs,\n    max_length=1024,\n    temperature=0.8,  # Increase temperature for more variability\n    top_k=50,         # Top-k sampling\n    top_p=0.9,        # Top-p (nucleus) sampling\n    repetition_penalty=1.2,  # Increase repetition penalty to avoid loops\n)\n\n# Decode and print the response\ndecoded_response = tokenizer.decode(response[0], skip_special_tokens=True)\nprint(decoded_response)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Set your Hugging Face token in the environment variables\nos.environ[\"HF_TOKEN\"] = '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'\n\n# Merge the LoRA adapters with the base model and push directly to the Hugging Face Hub\ntry:\n    model.push_to_hub_merged(\"AchrafGhribi31/llama3-esg-8b-merged\", tokenizer, save_method=\"merged_16bit\", token=os.environ.get(\"HF_TOKEN\"))\n    print(\"Merged model pushed to Hugging Face Hub.\")\nexcept Exception as e:\n    print(f\"Failed to push the merged model to Hugging Face: {e}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# List of questions about ESG reporting standards and KPIs\nquestions = [\n    \"What are the key ESG reporting standards companies should follow?\",\n    \"How can companies track and report their carbon footprint?\",\n    \"What are the most important ESG KPIs for the financial sector?\",\n    \"How do ESG reporting standards differ between regions?\",\n    \"What are the benefits of following the Global Reporting Initiative (GRI) standards?\",\n]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RAG Implementation\n","metadata":{}},{"cell_type":"code","source":"pip install torch transformers unsloth langchain faiss-gpu sentence-transformers \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install langchain_community","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import HuggingFacePipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\n# Create the text generation pipeline using your fine-tuned model\ntext_generation_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.1,\n    return_full_text=False,\n    max_new_tokens=300,\n    temperature=0.3,\n    do_sample=True\n)\n\n# Create the HuggingFacePipeline LLM\nfine_tuned_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\n# Load and process the CSV file\ndataset_path = '/kaggle/input/esgdata/CompaniesDataESG.csv'  # Replace with your actual file path\nloader = CSVLoader(file_path=dataset_path)\ndata = loader.load()\n\n# Split the documents into smaller chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\nchunked_docs = text_splitter.split_documents(data)\n\n# Create embeddings and build the FAISS index\nembeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\ndb = FAISS.from_documents(chunked_docs, embeddings)\n\n# Connect query to FAISS index using a retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={'k': 4}\n)\n\n# Create the Conversational Retrieval Chain\nqa_chain = ConversationalRetrievalChain.from_llm(\n    llm=fine_tuned_llm,\n    retriever=retriever,\n    return_source_documents=True\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\nchat_history = []\n\nwhile True:\n    query = input('Prompt (type \"exit\" to quit): ')\n    \n    if query.lower() == \"exit\":\n        print(\"Exiting the chat. Goodbye!\")\n        break\n    \n    # Determine if the query is general or company-specific\n    company_keywords = [\"company\", \"performance\", \"ESG risk\", \"rating\", \"sector\", \"employees\", \n                        \"decarbonization\", \"target\", \"turnover\", \"Name\", \"Ticker\", \"Sector\"]\n    \n    if any(keyword.lower() in query.lower() for keyword in company_keywords):\n        # Company-specific query, use the retriever\n        result = qa_chain.invoke({'question': query, 'chat_history': chat_history})\n        answer = result['answer']\n        \n    else:\n        # General ESG question, use the fine-tuned model directly\n        inputs = tokenizer(query, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(**inputs, max_length=512)\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Print the answer\n    print('Answer: ' + answer + '\\n')\n    \n    # Append the question and answer to the chat history\n    chat_history.append((query, answer))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\n\nfrom GPUtil import showUtilization as gpu_usage\ngpu_usage()       ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()                           \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BUILDING INTERFACE :","metadata":{}},{"cell_type":"code","source":"!pip install panel ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import panel as pn\nfrom panel.chat import ChatInterface\nfrom transformers import pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\n# Initialize Panel extension\npn.extension()\n\n# Create the text generation pipeline using your fine-tuned model\ntext_generation_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.1,\n    return_full_text=False,\n    max_new_tokens=300,\n    temperature=0.3,\n    do_sample=True\n)\n\n# Create the HuggingFacePipeline LLM\nfine_tuned_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\n# Load and process the CSV file for RAG\ndataset_path = '/kaggle/input/esgdata/CompaniesDataESG.csv'  # Replace with your actual file path\nloader = CSVLoader(file_path=dataset_path)\ndata = loader.load()\n\n# Split the documents into smaller chunks for better retrieval performance\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\nchunked_docs = text_splitter.split_documents(data)\n\n# Create embeddings and build the FAISS index\nembeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\ndb = FAISS.from_documents(chunked_docs, embeddings)\n\n# Connect the query to the FAISS index using a retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={'k': 4}\n)\n\n# Create the Conversational Retrieval Chain with the fine-tuned LLM\nqa_chain = ConversationalRetrievalChain.from_llm(\n    llm=fine_tuned_llm,\n    retriever=retriever,\n    return_source_documents=True\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the chatbot interaction function\ndef interact(query, chat_history=[]):\n    company_keywords = [\"company\", \"performance\", \"ESG risk\", \"rating\", \"sector\", \"employees\", \n                        \"decarbonization\", \"target\", \"turnover\", \"Name\", \"Ticker\", \"Sector\"]\n    \n    if any(keyword.lower() in query.lower()):\n        result = qa_chain.invoke({'question': query, 'chat_history': chat_history})\n        answer = result['answer']\n    else:\n        inputs = tokenizer(query, return_tensors=\"pt\").to(model.device)\n        outputs = model.generate(**inputs, max_length=512)\n        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    chat_history.append((query, answer))\n    return answer\n\n# Panel widgets for the chatbot interface\ninput_box = pn.widgets.TextInput(name=\"Enter your question:\", placeholder=\"Type your question here...\")\noutput_box = pn.pane.Markdown(\"\", height=300)\nsubmit_button = pn.widgets.Button(name=\"Submit\", button_type=\"primary\")\n\n# Define the function that handles user input and updates the chat\ndef on_submit(event):\n    user_query = input_box.value\n    if user_query:\n        response = interact(user_query)\n        current_text = output_box.object\n        output_box.object = f\"{current_text}\\n\\n**User:** {user_query}\\n**Bot:** {response}\"\n        input_box.value = \"\"  # Clear the input box after submission\n\n# Link the submit button to the on_submit function\nsubmit_button.on_click(on_submit)\n\n# Layout for the Panel app\nchat_interface = pn.Column(\n    pn.pane.Markdown(\"# ESG Chatbot Interface\"),\n    input_box,\n    submit_button,\n    output_box,\n)\n\n# Serve the application (do not specify port or address)\npn.serve(chat_interface)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dash\n","metadata":{}},{"cell_type":"code","source":"!pip install jupyter-dash\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install dash\n!pip install dash-bootstrap-components\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport dash\nfrom dash import dcc, html, Input, Output, State\nimport dash_bootstrap_components as dbc\nfrom unsloth import FastLanguageModel\nfrom transformers import pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\n# Initialize the Dash app with external stylesheet\napp = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n\n# Load your fine-tuned model and tokenizer using unsloth\nmodel_name = \"AchrafGhribi31/llama3-esg-8b-merged-4bit_V0\"\ntokenizer_name = model_name  # Use the same name for the tokenizer\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=1024,\n    load_in_4bit=True,\n    token=os.environ.get(\"HF_TOKEN\")\n)\n\n# Create the text generation pipeline using your fine-tuned model\ntext_generation_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.1,\n    return_full_text=False,\n    max_new_tokens=300,\n    temperature=0.3,\n    do_sample=True\n)\n\n# Create the HuggingFacePipeline LLM\nfine_tuned_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\n# Load and process the CSV file\ndataset_path = '/kaggle/input/esgdata/CompaniesDataESG.csv'  # Replace with your actual file path\nloader = CSVLoader(file_path=dataset_path)\ndata = loader.load()\n\n# Split the documents into smaller chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\nchunked_docs = text_splitter.split_documents(data)\n\n# Create embeddings and build the FAISS index\nembeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\ndb = FAISS.from_documents(chunked_docs, embeddings)\n\n# Connect query to FAISS index using a retriever\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={'k': 4}\n)\n\n# Create the Conversational Retrieval Chain\nqa_chain = ConversationalRetrievalChain.from_llm(\n    llm=fine_tuned_llm,\n    retriever=retriever,\n    return_source_documents=True\n)\n\n# Layout of the Dash app\napp.layout = dbc.Container([\n    dbc.Row(dbc.Col(html.H1(\"ESG Chatbot\"), className=\"text-center my-4\")),\n    dbc.Row(dbc.Col(dcc.Textarea(id=\"user_input\", placeholder=\"Ask a question...\", style={\"width\": \"100%\", \"height\": 100}), className=\"mb-3\")),\n    dbc.Row(dbc.Col(dbc.Button(\"Submit\", id=\"submit_button\", color=\"primary\"), className=\"mb-3 text-center\")),\n    dbc.Row(dbc.Col(html.Div(id=\"chat_output\"), className=\"mt-3\")),\n])\n\n# Callback to handle user input and generate responses\n@app.callback(\n    Output(\"chat_output\", \"children\"),\n    Input(\"submit_button\", \"n_clicks\"),\n    State(\"user_input\", \"value\"),\n)\ndef update_chat(n_clicks, user_input):\n    if n_clicks is None or user_input.strip() == \"\":\n        return \"\"\n\n    # Process the query\n    result = qa_chain({'question': user_input, 'chat_history': []})\n    answer = result['answer']\n    \n    # Display the answer in the app\n    return html.Div([\n        html.P(f\"User: {user_input}\"),\n        html.P(f\"Bot: {answer}\"),\n    ])\n\n# Run the app in the notebook\napp.run_server(host=\"0.0.0.0\", port=8050)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}